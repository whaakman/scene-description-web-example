{
    "runtime": "python",
    "version": "0.1.37",
    "trace": {
        "name": "run",
        "__time": {
            "start": "2024-11-27T07:48:54.125131",
            "end": "2024-11-27T07:48:59.178944",
            "duration": 5053
        },
        "signature": "__main__.SceneDescriptionAssistant.run",
        "inputs": {
            "question": [
                "a man in a white shirt",
                "a close up of a person's face with glasses",
                "a green scope with a helicopter in the background",
                "a black and gold logo",
                "a close up of a pair of glasses",
                "a close up of a person's lips",
                "a man in a white shirt",
                "a close-up of a person's face",
                "a close up of a person's face"
            ]
        },
        "__frames": [
            {
                "name": "execute",
                "__time": {
                    "start": "2024-11-27T07:48:54.125131",
                    "end": "2024-11-27T07:48:59.178944",
                    "duration": 5053
                },
                "signature": "prompty.execute",
                "description": "Execute a prompty",
                "inputs": {
                    "prompt": "imagecaption.prompty",
                    "configuration": {},
                    "parameters": {},
                    "inputs": {
                        "question": [
                            "a man in a white shirt",
                            "a close up of a person's face with glasses",
                            "a green scope with a helicopter in the background",
                            "a black and gold logo",
                            "a close up of a pair of glasses",
                            "a close up of a person's lips",
                            "a man in a white shirt",
                            "a close-up of a person's face",
                            "a close up of a person's face"
                        ]
                    },
                    "raw": false,
                    "config_name": "default"
                },
                "__frames": [
                    {
                        "name": "load",
                        "__time": {
                            "start": "2024-11-27T07:48:54.129493",
                            "end": "2024-11-27T07:48:54.132578",
                            "duration": 3
                        },
                        "signature": "prompty.load",
                        "description": "Load a prompty file.",
                        "inputs": {
                            "prompty_file": "C:\\Users\\whaak\\OneDrive\\Code\\ai-examples\\imagecaption-web\\imagecaption.prompty",
                            "configuration": "default"
                        },
                        "result": {
                            "name": "ExamplePrompt",
                            "description": "A prompt that uses context to ground an incoming question",
                            "authors": [
                                "Wesley Haakman"
                            ],
                            "model": {
                                "api": "chat",
                                "configuration": {
                                    "type": "azure_openai",
                                    "azure_endpoint": "https://ai-spirits-wus.openai.azure.com",
                                    "azure_deployment": "gpt-4",
                                    "api_version": "2024-02-15-preview"
                                },
                                "parameters": {
                                    "max_tokens": 3000
                                },
                                "response": {}
                            },
                            "sample": {
                                "question": "Frying pan, fondant, hamburger, whisk, and spatula"
                            },
                            "template": {
                                "type": "jinja2",
                                "parser": "prompty"
                            },
                            "file": "C:/Users/whaak/OneDrive/Code/ai-examples/imagecaption-web/imagecaption.prompty",
                            "content": "system:\nYou are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input.\n\nuser:\n{{question}}\n"
                        }
                    },
                    {
                        "name": "prepare",
                        "__time": {
                            "start": "2024-11-27T07:48:54.132578",
                            "end": "2024-11-27T07:48:54.135101",
                            "duration": 2
                        },
                        "signature": "prompty.prepare",
                        "description": "Prepare the inputs for the prompt.",
                        "inputs": {
                            "prompt": {
                                "name": "ExamplePrompt",
                                "description": "A prompt that uses context to ground an incoming question",
                                "authors": [
                                    "Wesley Haakman"
                                ],
                                "model": {
                                    "api": "chat",
                                    "configuration": {
                                        "type": "azure_openai",
                                        "azure_endpoint": "https://ai-spirits-wus.openai.azure.com",
                                        "azure_deployment": "gpt-4",
                                        "api_version": "2024-02-15-preview"
                                    },
                                    "parameters": {
                                        "max_tokens": 3000
                                    },
                                    "response": {}
                                },
                                "sample": {
                                    "question": "Frying pan, fondant, hamburger, whisk, and spatula"
                                },
                                "template": {
                                    "type": "jinja2",
                                    "parser": "prompty"
                                },
                                "file": "C:/Users/whaak/OneDrive/Code/ai-examples/imagecaption-web/imagecaption.prompty",
                                "content": "system:\nYou are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input.\n\nuser:\n{{question}}\n"
                            },
                            "inputs": {
                                "question": [
                                    "a man in a white shirt",
                                    "a close up of a person's face with glasses",
                                    "a green scope with a helicopter in the background",
                                    "a black and gold logo",
                                    "a close up of a pair of glasses",
                                    "a close up of a person's lips",
                                    "a man in a white shirt",
                                    "a close-up of a person's face",
                                    "a close up of a person's face"
                                ]
                            }
                        },
                        "__frames": [
                            {
                                "name": "Jinja2Renderer",
                                "__time": {
                                    "start": "2024-11-27T07:48:54.132578",
                                    "end": "2024-11-27T07:48:54.134096",
                                    "duration": 1
                                },
                                "signature": "prompty.renderers.Jinja2Renderer.invoke",
                                "inputs": {
                                    "data": {
                                        "question": [
                                            "a man in a white shirt",
                                            "a close up of a person's face with glasses",
                                            "a green scope with a helicopter in the background",
                                            "a black and gold logo",
                                            "a close up of a pair of glasses",
                                            "a close up of a person's lips",
                                            "a man in a white shirt",
                                            "a close-up of a person's face",
                                            "a close up of a person's face"
                                        ]
                                    }
                                },
                                "result": "system:\nYou are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input.\n\nuser:\n['a man in a white shirt', \"a close up of a person's face with glasses\", 'a green scope with a helicopter in the background', 'a black and gold logo', 'a close up of a pair of glasses', \"a close up of a person's lips\", 'a man in a white shirt', \"a close-up of a person's face\", \"a close up of a person's face\"]"
                            },
                            {
                                "name": "PromptyChatParser",
                                "__time": {
                                    "start": "2024-11-27T07:48:54.134096",
                                    "end": "2024-11-27T07:48:54.135101",
                                    "duration": 1
                                },
                                "signature": "prompty.parsers.PromptyChatParser.invoke",
                                "inputs": {
                                    "data": "system:\nYou are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input.\n\nuser:\n['a man in a white shirt', \"a close up of a person's face with glasses\", 'a green scope with a helicopter in the background', 'a black and gold logo', 'a close up of a pair of glasses', \"a close up of a person's lips\", 'a man in a white shirt', \"a close-up of a person's face\", \"a close up of a person's face\"]"
                                },
                                "result": [
                                    {
                                        "role": "system",
                                        "content": "You are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input."
                                    },
                                    {
                                        "role": "user",
                                        "content": "['a man in a white shirt', \"a close up of a person's face with glasses\", 'a green scope with a helicopter in the background', 'a black and gold logo', 'a close up of a pair of glasses', \"a close up of a person's lips\", 'a man in a white shirt', \"a close-up of a person's face\", \"a close up of a person's face\"]"
                                    }
                                ]
                            }
                        ],
                        "result": [
                            {
                                "role": "system",
                                "content": "You are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input."
                            },
                            {
                                "role": "user",
                                "content": "['a man in a white shirt', \"a close up of a person's face with glasses\", 'a green scope with a helicopter in the background', 'a black and gold logo', 'a close up of a pair of glasses', \"a close up of a person's lips\", 'a man in a white shirt', \"a close-up of a person's face\", \"a close up of a person's face\"]"
                            }
                        ]
                    },
                    {
                        "name": "run",
                        "__time": {
                            "start": "2024-11-27T07:48:54.135101",
                            "end": "2024-11-27T07:48:59.177353",
                            "duration": 5042
                        },
                        "signature": "prompty.run",
                        "description": "Run the prepared Prompty content against the model.",
                        "inputs": {
                            "prompt": {
                                "name": "ExamplePrompt",
                                "description": "A prompt that uses context to ground an incoming question",
                                "authors": [
                                    "Wesley Haakman"
                                ],
                                "model": {
                                    "api": "chat",
                                    "configuration": {
                                        "type": "azure_openai",
                                        "azure_endpoint": "https://ai-spirits-wus.openai.azure.com",
                                        "azure_deployment": "gpt-4",
                                        "api_version": "2024-02-15-preview"
                                    },
                                    "parameters": {
                                        "max_tokens": 3000
                                    },
                                    "response": {}
                                },
                                "sample": {
                                    "question": "Frying pan, fondant, hamburger, whisk, and spatula"
                                },
                                "template": {
                                    "type": "jinja2",
                                    "parser": "prompty"
                                },
                                "file": "C:/Users/whaak/OneDrive/Code/ai-examples/imagecaption-web/imagecaption.prompty",
                                "content": "system:\nYou are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input.\n\nuser:\n{{question}}\n"
                            },
                            "content": [
                                {
                                    "role": "system",
                                    "content": "You are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input."
                                },
                                {
                                    "role": "user",
                                    "content": "['a man in a white shirt', \"a close up of a person's face with glasses\", 'a green scope with a helicopter in the background', 'a black and gold logo', 'a close up of a pair of glasses', \"a close up of a person's lips\", 'a man in a white shirt', \"a close-up of a person's face\", \"a close up of a person's face\"]"
                                }
                            ],
                            "configuration": {},
                            "parameters": {},
                            "raw": false
                        },
                        "__frames": [
                            {
                                "name": "AzureOpenAIExecutor",
                                "__time": {
                                    "start": "2024-11-27T07:48:54.140331",
                                    "end": "2024-11-27T07:48:59.176849",
                                    "duration": 5036
                                },
                                "signature": "prompty.azure.executor.AzureOpenAIExecutor.invoke",
                                "inputs": {
                                    "data": [
                                        {
                                            "role": "system",
                                            "content": "You are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input."
                                        },
                                        {
                                            "role": "user",
                                            "content": "['a man in a white shirt', \"a close up of a person's face with glasses\", 'a green scope with a helicopter in the background', 'a black and gold logo', 'a close up of a pair of glasses', \"a close up of a person's lips\", 'a man in a white shirt', \"a close-up of a person's face\", \"a close up of a person's face\"]"
                                        }
                                    ]
                                },
                                "__frames": [
                                    {
                                        "name": "AzureOpenAI",
                                        "__time": {
                                            "start": "2024-11-27T07:48:54.140331",
                                            "end": "2024-11-27T07:48:54.391166",
                                            "duration": 250
                                        },
                                        "type": "LLM",
                                        "signature": "AzureOpenAI.ctor",
                                        "description": "Azure OpenAI Constructor",
                                        "inputs": {
                                            "azure_endpoint": "https://ai-spirits-wus.openai.azure.com",
                                            "azure_deployment": "gpt-4",
                                            "api_version": "2024-02-15-preview",
                                            "azure_ad_token_provider": "***************************************************************************"
                                        },
                                        "result": "<openai.lib.azure.AzureOpenAI object at 0x000001FC74FF8BC0>"
                                    },
                                    {
                                        "name": "create",
                                        "__time": {
                                            "start": "2024-11-27T07:48:54.391166",
                                            "end": "2024-11-27T07:48:59.140804",
                                            "duration": 4749
                                        },
                                        "type": "LLM",
                                        "description": "Azure OpenAI Client",
                                        "signature": "AzureOpenAI.chat.completions.create",
                                        "inputs": {
                                            "model": "gpt-4",
                                            "messages": [
                                                {
                                                    "role": "system",
                                                    "content": "You are an assistant that based on a prompt will describe the situation. The input you receive are objects detected by a computer vision model. Describe what is happening based on the input."
                                                },
                                                {
                                                    "role": "user",
                                                    "content": "['a man in a white shirt', \"a close up of a person's face with glasses\", 'a green scope with a helicopter in the background', 'a black and gold logo', 'a close up of a pair of glasses', \"a close up of a person's lips\", 'a man in a white shirt', \"a close-up of a person's face\", \"a close up of a person's face\"]"
                                                }
                                            ],
                                            "max_tokens": 3000
                                        }
                                    }
                                ],
                                "result": {
                                    "exception": {
                                        "type": "<class 'openai.AuthenticationError'>",
                                        "traceback": [
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 69, in run\n    return self.invoke(data)\n           ^^^^^^^^^^^^^^^^^\n",
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\azure\\executor.py\", line 89, in invoke\n    response = client.chat.completions.create(**args)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n",
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n    return self._post(\n           ^^^^^^^^^^^\n",
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n",
                                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n    raise self._make_status_error_from_response(err.response) from None\n"
                                        ],
                                        "message": "Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}",
                                        "args": "(\"Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}\",)"
                                    }
                                }
                            }
                        ],
                        "result": {
                            "exception": {
                                "type": "<class 'openai.AuthenticationError'>",
                                "traceback": [
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\__init__.py\", line 424, in run\n    result = InvokerFactory.run_executor(prompt, content)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 255, in run_executor\n    return cls.run(\"executor\", prompty, data, default)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 213, in run\n    value = invoker.run(data)\n            ^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 167, in wrapper\n    raise e\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 69, in run\n    return self.invoke(data)\n           ^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\azure\\executor.py\", line 89, in invoke\n    response = client.chat.completions.create(**args)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n    return self._post(\n           ^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n",
                                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n    raise self._make_status_error_from_response(err.response) from None\n"
                                ],
                                "message": "Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}",
                                "args": "(\"Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}\",)"
                            }
                        }
                    }
                ],
                "result": {
                    "exception": {
                        "type": "<class 'openai.AuthenticationError'>",
                        "traceback": [
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\__init__.py\", line 532, in execute\n    result = run(prompt, content, configuration, parameters, raw)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 167, in wrapper\n    raise e\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\__init__.py\", line 424, in run\n    result = InvokerFactory.run_executor(prompt, content)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 255, in run_executor\n    return cls.run(\"executor\", prompty, data, default)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 213, in run\n    value = invoker.run(data)\n            ^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 167, in wrapper\n    raise e\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 69, in run\n    return self.invoke(data)\n           ^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\azure\\executor.py\", line 89, in invoke\n    response = client.chat.completions.create(**args)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n    return self._post(\n           ^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n",
                            "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n    raise self._make_status_error_from_response(err.response) from None\n"
                        ],
                        "message": "Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}",
                        "args": "(\"Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}\",)"
                    }
                }
            }
        ],
        "result": {
            "exception": {
                "type": "<class 'openai.AuthenticationError'>",
                "traceback": [
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\OneDrive\\Code\\ai-examples\\imagecaption-web\\imagecaption.py\", line 107, in run\n    result = prompty.execute(\n             ^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 167, in wrapper\n    raise e\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\__init__.py\", line 532, in execute\n    result = run(prompt, content, configuration, parameters, raw)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 167, in wrapper\n    raise e\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\__init__.py\", line 424, in run\n    result = InvokerFactory.run_executor(prompt, content)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 255, in run_executor\n    return cls.run(\"executor\", prompty, data, default)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 213, in run\n    value = invoker.run(data)\n            ^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 167, in wrapper\n    raise e\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\tracer.py\", line 149, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\invoker.py\", line 69, in run\n    return self.invoke(data)\n           ^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\prompty\\azure\\executor.py\", line 89, in invoke\n    response = client.chat.completions.create(**args)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n    return self._post(\n           ^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n    return self._request(\n           ^^^^^^^^^^^^^^\n",
                    "  File \"C:\\Users\\whaak\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n    raise self._make_status_error_from_response(err.response) from None\n"
                ],
                "message": "Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}",
                "args": "(\"Error code: 401 - {'error': {'code': 'PermissionDenied', 'message': 'Principal does not have access to API/Operation.'}}\",)"
            }
        }
    }
}